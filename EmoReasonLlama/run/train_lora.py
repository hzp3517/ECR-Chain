# Usage: deepspeed train.py --deepspeed <$PATH_TO_DEEPSPEED_CONFIG>

import copy
from typing import Any, Dict, List, Optional, Union, Sequence
from dataclasses import dataclass, field
import logging
import json
import pathlib
import typing
import os
from functools import partial

import numpy as np
import torch
from torch.utils.data import Dataset, ConcatDataset
from deepspeed import zero
from deepspeed.runtime.zero.partition_parameters import ZeroParamStatus
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import transformers
from transformers import Trainer
from transformers import BitsAndBytesConfig, deepspeed
import math
import copy

from EmoReasonLlama.data import *
from EmoReasonLlama.model.llama_for_causal_lm import LlamaForCausalLM
from EmoReasonLlama.run.trainer import EmoReasonTrainer, LogCallback
from EmoReasonLlama.utils import set_random_seed

# ------- start: 添加opts --------

# 注意：这里但凡设置了默认值的参数，必须放置在其它未设置默认值参数的后面！否则会报错。

@dataclass
class ModelArguments:
    model_name_or_path: Optional[str] = field(default="/data4/hzp/pretrained_models/llama/llama-7b-hf/")
    padding_side: str = field(
        default='right',
        metadata={"help": "`padding_side` param for the tokenizer. \
                    The default padding_side of Stanford-Alpaca and FastChat (Vicuna) is `right` (Alpaca-lora is `left`). \
                    However, if we want to realize batch inference, it's better to set it to `left`", 
                  "choices": ['left', 'right']}
    )


@dataclass
class DataArguments:
    raw_data_path: Optional[str] = field(
        default='/data4/hzp/datasets/MECPE/process_data/RECCON_all_data.json',
        metadata={"help": "Path to the input file"}
    )
    training_chains_path: Optional[str] = field(
        default='/data4/hzp/ECR-Chain/chatgpt/prompt_examples/CEE_backward_v2_fewshot/4shot_train/complete_subset.json',
        metadata={"help": "Path to the file of reasoning chains of training samples (automatically generated by chatgpt)"}
    )
    cot_gene_type: Optional[str] = field(
        default=None,
        metadata={"help": "Where the cot knowledge from, ['chatgpt', 'vicuna']"}
    )
    cot_gene_dir: Optional[str] = field(
        default=None,
        metadata={"help": "The inference_results dir of cot generation model if `cot_gene_type` is 'vicuna'."}
    )
    explanation_path: Optional[str] = field(
        default='/data4/hzp/ECR-Chain/chatgpt/prompt_examples/CEE_supp_fewshot/4shot_train/explain_subset.json',
        metadata={"help": "Path to the file of explanations for pos or neg samples (automatically generated by chatgpt)"}
    )
    conv_templ_type: str = field(
        default='vicuna_v1.1',
        metadata={"help": "Specify a conversation template", 
                  "choices": ['alpaca', 'vicuna_v1.1']}
    )
    # task_templ_type: str = field(
    #     default='CEE_vanilla',
    #     metadata={"help": "Specify task prompt and response template for the target task"}
    # )
    main_task_templ: str = field(
        default='CEE_vanilla',
        metadata={"help": "Specify task prompt and response template for the target task"}
    )
    aux_task_templ_list: Optional[str] = field(
        default=None,
        metadata={"help": "Auxiliary training tasks, splited by comma. Default is None."}
    )
    example_id: str = field(
        default='tr_264_6',
        metadata={"help": "The sample id of the exemplar selected from the train set"}
    )
    max_remain_utts: int = field(
        default=-1,
        metadata={"help": "Delete some utts early in the conversation to control the len of the overall input. -1 means not set."}
    )
    # lazy_preprocess: bool = False


@dataclass
class TrainingArguments(transformers.TrainingArguments):
    cache_dir: Optional[str] = field(default=None)
    optim: str = field(default="adamw_torch")
    model_max_length: int = field( # alpaca: 512, vicuna: 2048
        default=2048,
        metadata={
            "help": "Maximum sequence length. Sequences will be right padded (and possibly truncated)."
        },
    )
    seed: int = field(
        default=42,
        metadata={
            "help": "Set the value of random seed."
        },
    )

    # remove_unused_columns: Optional[bool] = field(
    #     default=True, metadata={"help": "Remove columns not required by the model when using an nlp.Dataset."}
    # )
    remove_unused_columns: Optional[bool] = field(
        default=False, metadata={"help": "Remove columns not required by the model when using an nlp.Dataset."}
    ) # transformers的TrainingArguments中默认是True，但是这里需要改成False，否则自定义的Dataset加载会有问题

    # **** hzp add for inference, evaluate and save during the training process ****
    num_beams: int = field(
        default=1,
        metadata={"help": "Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search."}
    )
    temperature: float = field(
        default=1.0,
        metadata={"help": "The value used to module the next token probabilities. Must be strictly positive."}
    ) # 理论上0代表greedy decoding，但是generate()不支持等于0，可以设一个小量（如1e-5）。
    top_k: int = field(
        default=50,
        metadata={"help": "The number of highest probability vocabulary tokens to keep for top-k-filtering."}
    )
    top_p: float = field(
        default=1.0,
        metadata={"help": "If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation."}
    )
    max_new_tokens: int = field(
        default=128,
        metadata={"help": "The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt."}
    )
    save_predictions: bool = field(
        default=True,
        metadata={"help": "If set it `True`, save the predictions into json files."}
    )
    save_metrics: bool = field(
        default=True,
        metadata={"help": "If set it `True`, save the result dict into json files."}
    )

    # for multitask training:
    aux_task_distrib_list: Optional[str] = field(
        default=None,
        metadata={"help": "List of int numbers splited by commas (e.g. '1,2,1'). Represents the distribution of each aux task: \
                  When running the main task `x` times, run the auxiliary task once. \
                  The length of the list and the order of each element should be consistent with `aux_task_templ_list`."}
    )
    aux_task_position_list: Optional[str] = field(
        default=None,
        metadata={"help": "List of int numbers splited by commas (e.g. '0,1,0'). Represents when to run the auxiliary task (when the `distrib` of this task > 1) \
                  e.g. for task t_1, its `distrib` is 2, its `position` is 1. \
                  - `distrib`==2 means: Run the t_1 once as running the main task `x` times. \
                  - `position`==1 means: When the iter number is `i` and `i` % 2 == 1, we run the t_1. \
                  The length of the list and the order of each element should be consistent with `aux_task_templ_list`."}
    )
    aux_task_weight_list: Optional[str] = field(
        default=None,
        metadata={"help": "List of float numbers splited by commas (e.g. '1.0,2.0,1.0'). Each element represents the loss weight of each task. \
                  The length of the list and the order of each element should be consistent with `aux_task_templ_list`."}
    )
    # ******************************************************************************



@dataclass
class LoraArguments:
    lora_r: int = 8
    lora_alpha: int = 16
    lora_dropout: float = 0.05
    lora_target_modules: typing.List[str] = field(
        default_factory=lambda: ["q_proj", "v_proj"]
    )
    lora_weight_path: str = ""
    lora_bias: str = "none"
    q_lora: bool = False

# ------- end: 添加opts --------


def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):
    """Collects the state dict and dump to disk."""
    state_dict = trainer.model.state_dict()
    if trainer.args.should_save:
        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}
        del state_dict
        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa


def maybe_zero_3(param):
    if hasattr(param, "ds_id"):
        assert param.ds_status == ZeroParamStatus.NOT_AVAILABLE
        with zero.GatheredParameters([param]):
            param = param.data.detach().cpu().clone()
    else:
        param = param.detach().cpu().clone()
    return param


# Borrowed from peft.utils.get_peft_model_state_dict
def get_peft_state_maybe_zero_3(named_params, bias):
    if bias == "none":
        to_return = {k: t for k, t in named_params if "lora_" in k}
    elif bias == "all":
        to_return = {k: t for k, t in named_params if "lora_" in k or "bias" in k}
    elif bias == "lora_only":
        to_return = {}
        maybe_lora_bias = {}
        lora_bias_names = set()
        for k, t in named_params:
            if "lora_" in k:
                to_return[k] = t
                bias_name = k.split("lora_")[0] + "bias"
                lora_bias_names.add(bias_name)
            elif "bias" in k:
                maybe_lora_bias[k] = t
        for k, t in maybe_lora_bias:
            if bias_name in lora_bias_names:
                to_return[bias_name] = t
    else:
        raise NotImplementedError
    to_return = {k: maybe_zero_3(v) for k, v in to_return.items()}
    return to_return


def train():
    parser = transformers.HfArgumentParser(
        (ModelArguments, DataArguments, TrainingArguments, LoraArguments)
    )
    (
        model_args,
        data_args,
        training_args,
        lora_args,
    ) = parser.parse_args_into_dataclasses()

    set_random_seed(training_args.seed)

    device_map = None
    if lora_args.q_lora:
        world_size = int(os.environ.get("WORLD_SIZE", 1))
        device_map = (
            {"": int(os.environ.get("LOCAL_RANK") or 0)} if world_size != 1 else None
        )
        if len(training_args.fsdp) > 0 or deepspeed.is_deepspeed_zero3_enabled():
            logging.warn("FSDP and ZeRO3 are both currently incompatible with QLoRA.")

    compute_dtype = (
        torch.float16
        if training_args.fp16
        else (torch.bfloat16 if training_args.bf16 else torch.float32)
    )

    # model = transformers.AutoModelForCausalLM.from_pretrained(
    model = LlamaForCausalLM.from_pretrained(
        model_args.model_name_or_path,
        cache_dir=training_args.cache_dir,
        device_map=device_map,
        quantization_config=BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=compute_dtype,
        )
        if lora_args.q_lora
        else None,
    )
    lora_config = LoraConfig(
        r=lora_args.lora_r,
        lora_alpha=lora_args.lora_alpha,
        target_modules=lora_args.lora_target_modules,
        lora_dropout=lora_args.lora_dropout,
        bias=lora_args.lora_bias,
        task_type="CAUSAL_LM",
    )

    if lora_args.q_lora:
        model = prepare_model_for_kbit_training(
            model, use_gradient_checkpointing=training_args.gradient_checkpointing
        )
        if torch.cuda.device_count() > 1:
            # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available
            model.is_parallelizable = True
            model.model_parallel = True

    model = get_peft_model(model, lora_config)
    if training_args.deepspeed is not None and training_args.local_rank == 0:
        model.print_trainable_parameters()

    if training_args.gradient_checkpointing:
        model.enable_input_require_grads()

    tokenizer = transformers.AutoTokenizer.from_pretrained(
        model_args.model_name_or_path,
        cache_dir=training_args.cache_dir,
        model_max_length=training_args.model_max_length,
        padding_side=model_args.padding_side,
        use_fast=False,
    )
    tokenizer.pad_token = tokenizer.unk_token

    # ----- load dataset and data_collator -----
    main_task_templ = data_args.main_task_templ
    if data_args.aux_task_templ_list and data_args.aux_task_templ_list != '':
        aux_task_templ_list = data_args.aux_task_templ_list.strip().split(',')
    else:
        aux_task_templ_list = []

    # for main task:
    main_task = None
    if main_task_templ == 'CEE_vanilla':
        train_dataset = CEECausalLMDataset('train', tokenizer, data_args.raw_data_path, data_args.conv_templ_type, main_task_templ, local_rank=0)
        dev_dataset = CEECausalLMDataset('dev', tokenizer, data_args.raw_data_path, data_args.conv_templ_type, main_task_templ, local_rank=0)
        test_dataset = CEECausalLMDataset('test', tokenizer, data_args.raw_data_path, data_args.conv_templ_type, main_task_templ, local_rank=0)
        collate_fn = CEECausalLMDataset.collate_fn
        main_task = 'answer'
    elif main_task_templ == 'CEE_simplify_1shot':
        train_dataset = CEEBackwardFilterCausalLMDataset('train', tokenizer, data_args.raw_data_path, data_args.training_chains_path, data_args.conv_templ_type, main_task_templ, data_args.example_id, data_args.max_remain_utts, local_rank=0)
        dev_dataset = CEEBackwardFilterCausalLMDataset('dev', tokenizer, data_args.raw_data_path, data_args.training_chains_path, data_args.conv_templ_type, main_task_templ, data_args.example_id, data_args.max_remain_utts, local_rank=0)
        test_dataset = CEEBackwardFilterCausalLMDataset('test', tokenizer, data_args.raw_data_path, data_args.training_chains_path, data_args.conv_templ_type, main_task_templ, data_args.example_id, data_args.max_remain_utts, local_rank=0)
        collate_fn = CEEBackwardFilterCausalLMDataset.collate_fn
        main_task = 'cot'
    else:
        raise Exception('Invalid main_task_templ!')

    # for aux tasks:
    len_main_dataset = len(train_dataset)
    aux_train_dataset_list = []
    aux_collate_fn_list = []
    aux_task_list = []
    if len(aux_task_templ_list):
        for aux_task_templ in aux_task_templ_list:
            if aux_task_templ == 'CEE_vanilla':
                assert 'answer' not in aux_task_list
                aux_task_list.append('answer')
                tmp_train_dataset = CEECausalLMDataset('train', tokenizer, data_args.raw_data_path, data_args.conv_templ_type, aux_task_templ, local_rank=0)
                aux_train_dataset_list.append(tmp_train_dataset)
                aux_collate_fn_list.append(CEECausalLMDataset.collate_fn)
            elif aux_task_templ.startswith('CEE_simplify_'):
                assert 'cot' not in aux_task_list
                aux_task_list.append('cot')
                tmp_train_dataset = CEEBackwardFilterCausalLMDataset('train', tokenizer, data_args.raw_data_path, data_args.training_chains_path, data_args.conv_templ_type, aux_task_templ, data_args.example_id, data_args.max_remain_utts, local_rank=0)
                aux_train_dataset_list.append(tmp_train_dataset)
                aux_collate_fn_list.append(CEEBackwardFilterCausalLMDataset.collate_fn)
            else:
                raise Exception('Invalid task_templ_type!')

    eval_datasets_dict = {'dev': dev_dataset, 'test': test_dataset}###
    # eval_datasets_dict = {'dev': dev_dataset}###

    setattr(training_args, "main_task", main_task)
    setattr(training_args, "aux_task_list", aux_task_list)
    if len(aux_task_templ_list):
        training_args.aux_task_distrib_list = [int(i.strip()) for i in training_args.aux_task_distrib_list.split(',')]
        if training_args.aux_task_position_list and len(training_args.aux_task_position_list):
            tmp_aux_task_position_list = [int(i.strip()) for i in training_args.aux_task_position_list.split(',')]
            training_args.aux_task_position_list = [tmp_aux_task_position_list[i] % training_args.aux_task_distrib_list[i] for i in range(len(training_args.aux_task_distrib_list))]
        else:
            training_args.aux_task_position_list = [0] * len(aux_task_templ_list)
        training_args.aux_task_weight_list = [float(i.strip()) for i in training_args.aux_task_weight_list.split(',')]
    else:
        training_args.aux_task_distrib_list = []
        training_args.aux_task_position_list = []
        training_args.aux_task_weight_list = []
    # ------------------------------------------

    trainer = EmoReasonTrainer(
        model=model, 
        tokenizer=tokenizer, 
        args=training_args,
        lora_args=lora_args,
        train_dataset=train_dataset,
        aux_train_dataset_list=aux_train_dataset_list,
        eval_dataset=eval_datasets_dict, # perform evaluation on every datasets in the dict
        data_collator=collate_fn,
        aux_collate_fn_list=aux_collate_fn_list,
        callbacks=[LogCallback()]
    )

    model.config.use_cache = False

    if list(pathlib.Path(training_args.output_dir).glob("checkpoint-*")):
        trainer.train(resume_from_checkpoint=True)
    else:
        trainer.train()
    trainer.save_state()

    # check if zero3 mode enabled
    if trainer.hf_deepspeed_config_orig.is_zero3():
        # use deepspeed engine internal function to gather state dict
        # state_dict_zero3 contains whole parameters of base and lora adapters
        # we will not extract lora parameters since peft save_pretrained will do that
        # https://github.com/huggingface/peft/blob/3714aa2fff158fdfa637b2b65952580801d890b2/src/peft/peft_model.py#L125
        # https://github.com/huggingface/peft/blob/3714aa2fff158fdfa637b2b65952580801d890b2/src/peft/utils/save_and_load.py#L19
        state_dict_zero3 = trainer.model_wrapped._zero3_consolidated_16bit_state_dict()
        if training_args.local_rank == 0:
            state_dict = state_dict_zero3
    else:
        # in other mode we use original code from fastchat team, to make sure our change is minimum
        state_dict = get_peft_state_maybe_zero_3(
            model.named_parameters(), lora_args.lora_bias
        )

    if training_args.local_rank == 0:
        model.save_pretrained(training_args.output_dir, state_dict=state_dict)


if __name__ == "__main__":
    # -----make wandb offline------
    import os
    os.environ["WANDB_MODE"] = "offline"
    # -----------------------------

    train()